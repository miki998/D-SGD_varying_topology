{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be87c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce4e96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardTopology(K, standardTopo):\n",
    "    custom = CustomTopology(K)\n",
    "    A = nx.adjacency_matrix(standardTopo(K).to_networkx()).toarray()\n",
    "    custom.set_neighbours(A)\n",
    "    custom.to_networkx()\n",
    "    return custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8026e7",
   "metadata": {},
   "source": [
    "### 1. Retrieve end topology with logs from topology search\n",
    "We use the logs to retrieve the topologies found from the iterative method. We use that topology for following experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f11420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataset from MNIST torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "datasetMNIST  = datasets.MNIST(root=\"./\", download=True)\n",
    "load = Loader()\n",
    "mnist, mnistL = load.toArray(datasetMNIST)\n",
    "mnist, mnistL = (torch.tensor(mnist).float(), torch.tensor(mnistL))\n",
    "print('shape of dataset from MNIST {}'.format(mnist.shape))\n",
    "trainData, trainL= mnist[:-20000], mnistL[:-20000]\n",
    "valData , valL   = mnist[-20000:-10000], mnistL[-20000:-10000]\n",
    "testData, testL  = mnist[-10000:], mnistL[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "309ef2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 3\n",
    "nbEpoch    = 100\n",
    "batchSize  = 100\n",
    "np.random.seed(s)\n",
    "nodesNoneIIDData = []\n",
    "indexIID = mnist_noniid(trainData, trainL, 8)\n",
    "\n",
    "for i in range(8):\n",
    "    tmpX = mnist[indexIID[i]].reshape(-1, batchSize, 1, 28,28)\n",
    "    tmpy = mnistL[indexIID[i]].reshape(-1, batchSize)\n",
    "    nodesNoneIIDData.append((tmpX,tmpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf2585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./logs/scoresValImprovedBinary.pkl','rb') as f:\n",
    "    valImp   = pickle.load(f)\n",
    "    \n",
    "startingTopology = BinaryTreeTopology\n",
    "topology = standardTopology(8, startingTopology)\n",
    "\n",
    "topologies = []\n",
    "for resultdfVal in valImp[s]:\n",
    "    topologies.append(deepcopy(topology))\n",
    "    nodeMin     = np.argmin(resultdfVal[resultdfVal['Epoch']==nbEpoch]['acc'])\n",
    "    node2Change = resultdfVal[resultdfVal['Epoch']==nbEpoch].iloc[nodeMin]['node#']\n",
    "\n",
    "    # Let's find a non neighbour node to add into the neighbours \n",
    "    # so as to perhaps have better performance later on\n",
    "    cur_neighbours = topology.get_neighbors(int(node2Change))\n",
    "    exist          = set()\n",
    "    for cur in cur_neighbours:\n",
    "        exist      = exist.union(set(np.array(nodesNoneIIDData[cur][1].flatten())))\n",
    "\n",
    "    non_neighbours = list(set(range(8)) - set(cur_neighbours) - set([int(node2Change)]))\n",
    "    if len(non_neighbours) == 0: break\n",
    "    idxBest        = np.argmax([len(exist.union(set(np.array(nodesNoneIIDData[noncur][1].flatten())))) \n",
    "                       for noncur in non_neighbours])\n",
    "    node2attach    = non_neighbours[idxBest]\n",
    "\n",
    "    topology.adjency[int(node2Change),node2attach] = 1\n",
    "    topology.adjency[node2attach,int(node2Change)] = 1\n",
    "    topology.to_networkx()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876500c",
   "metadata": {},
   "source": [
    "### 2. Performance evaluation on 10 runs with fixed distribution but 10 seeds for model weight initialization and D-SGD process\n",
    "Previously we computed 10 runs on both seeded 10 distributions and training runs so as to not overfit on one of distribution. After finding a decent topology we use do once again 10 runs with a fixed distribution to verify the better accuracy in both validation and test dataset that the new topology yields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cadae491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5096bd354a2d4bfeb752660b7595f92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5f3603992d45be87395aa2acdb57dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8281cee911bc4630a5ce1d0cc89c5bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495ee327f71743b59a8b5a9f981af0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e953adeaa2c14d1498b1b680567e5d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35a718704784a4f8df3a41b5cd3f2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849b7c3d09254b2eac1786e81f83f671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095cf7cf688b42bf8042c371e99b4b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4cd2da4f954095a36f7a17af69161b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be94d9f28bd4d05a9ba9e6b237e66c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9458e30ebae44fa8b65f72aa1363075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scoresTrain  = []\n",
    "scoresVal    = []\n",
    "scoresTest   = []\n",
    "scoresDist   = []\n",
    "\n",
    "# PARAM DEFINING \n",
    "K = 8 # number of nodes\n",
    "nbEpoch    = 100\n",
    "batchSize  = 100\n",
    "nodesValidation = (valData.reshape(-1, valData.shape[0], 1, 28, 28), valL.reshape(-1,valL.shape[0]))\n",
    "nodesTest       = (testData.reshape(-1, testData.shape[0], 1, 28, 28), testL.reshape(-1,testL.shape[0]))\n",
    "\n",
    "\n",
    "# Generating NON-IID Dataset distribution\n",
    "nodesNoneIIDData = []\n",
    "np.random.seed(s)\n",
    "indexIID = mnist_noniid(trainData, trainL, K)\n",
    "\n",
    "for i in range(K):\n",
    "    tmpX = mnist[indexIID[i]].reshape(-1, batchSize, 1, 28,28)\n",
    "    tmpy = mnistL[indexIID[i]].reshape(-1, batchSize)\n",
    "    nodesNoneIIDData.append((tmpX,tmpy))\n",
    "\n",
    "# LOOP FOR STATIC TOPOLOGY\n",
    "for t in tqdm(range(10)):\n",
    "    torch.manual_seed(t) # fix seed for model training\n",
    "    resultsTrain = []\n",
    "    resultsVal   = []\n",
    "    resultsDist  = []    \n",
    "    resultsModel = []\n",
    "    ret   = []    \n",
    "    for k in [0]: # use selected 6th topology, and so a topology with ring + 5 edges\n",
    "\n",
    "        # Reset topology that we improve\n",
    "        topology = TorusTopology(K, dimension='3d')\n",
    "\n",
    "        resultdfTrain, resultdfVal, cdist, models = computeDecentralize(nodesNoneIIDData, topology, \n",
    "                                                            K, max_epoch=nbEpoch,\n",
    "                                                           validation=nodesValidation)\n",
    "        resultsTrain.append(resultdfTrain)\n",
    "        resultsVal.append(resultdfVal)\n",
    "        resultsDist.append(cdist)\n",
    "        resultsModel.append(models)\n",
    "\n",
    "        # Test accuracy with obtained model\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        graph   = topology.to_networkx()\n",
    "\n",
    "        test_stats = {str(node):[] for node in graph.nodes()}\n",
    "        for node_idx in graph.nodes():\n",
    "\n",
    "            lossTest, accTest = subopt(nodesTest, models[node_idx], loss_fn)\n",
    "            curTestDir        = {'loss': lossTest, 'acc': accTest}\n",
    "            test_stats[str(node_idx)].append(curTestDir)\n",
    "\n",
    "        resultdfTest = getLogs(test_stats)\n",
    "        ret.append(resultdfTest)\n",
    "\n",
    "    # LOGGING THE RESULTS\n",
    "    scoresTrain.append(resultsTrain)\n",
    "    scoresVal.append(resultsVal)\n",
    "    scoresDist.append(resultsDist)\n",
    "    scoresTest.append(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebf67d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./trainingLogs/scoresTrainTorus.pkl','wb') as f:\n",
    "    pickle.dump(scoresTrain,f)\n",
    "with open('./trainingLogs/scoresValTorus.pkl','wb') as f:\n",
    "    pickle.dump(scoresVal,f)\n",
    "with open('./trainingLogs/scoresTestTorus.pkl','wb') as f:\n",
    "    pickle.dump(scoresTest,f)\n",
    "with open('./trainingLogs/scoresDistTorus.pkl','wb') as f:\n",
    "    pickle.dump(scoresDist,f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090cd45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
